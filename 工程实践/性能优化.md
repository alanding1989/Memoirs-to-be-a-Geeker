
<!-- vim-markdown-toc GFM -->

- [常用的性能优化方法](#常用的性能优化方法)
  - [按照以下步骤优化:](#按照以下步骤优化)

<!-- vim-markdown-toc -->


### 常用的性能优化方法

1. 考虑优化算法
2. 考虑程序优化.  
3. 考虑升级机器，多搞点内存，然后东西尽量放在内存里搞.  
4. 考虑并行，找个hadoop集群，写成mapreduce程序跑 放在hadoop上跑.
5. 考虑拿C或C++重写.  

- 单机情况下:  
  首先减少输入数据，然后不要浪费机器资源，要让所有CPU核心跑满（多进程 & 减少/不等待IO）.  
  内存只要还够用的话，就用！ 
  然后找程序最慢的地方，针对其做各种优化.  
- 多机弄到hadoop里搞，数据再多也不怕.


#### 按照以下步骤优化:  

1. 首先，确信你真的需要把全部数据过一遍，如果可以通过一些糙快猛方式过滤掉无用数据，这样最好了.  
（比如有些明显无用的东西可以直接通过grep过滤掉，grep这种程序写的一般比你写的python程序要快好多好多好多好多).  

2. top看CPU跑满了没.单线程单进程实现能不能搞成多进程的？然后top看每个核都跑满了吗？要充分利用CPU， 
看看程序，没跑满是因为IO吗？是的话IO能搞成异步的么？ 
或IO次数太多？能不能减少IO次数？甚至只搞一次IO，比如1G的东西，能不能一次全搞到内存里，然后所有东西在内存里处理. 
（这样的话貌似写成C的更方便一点）.  

3. 如果每个核心都跑满了，那就看看计算都花在什么地方.可以用profile, hotshot等工具测一把. 可以粗略比较一下在  
1/16 数据、1/8数据、1/4数据、1/2数据的情况下hotshot的结果，看函数花的时间是怎么涨的.  
找出花时间最多的一个或几个东西（所谓瓶颈），有针对性优化可以事半功倍.

4. 找到问题所在之后，寻求解决方案.  
   - 如果是python带的数据结构不合适，能不能用numpy之类的东西解决，能不能用一些数据库解决.  
   - 比如需要多个进程一起往一个大字典里写，可以考虑全往一个redis里写，任务队列，内存缓冲.  
   - 能不能有的地方用cython包装一个C实现.   
   - 如果是算法不够好，能不能优化算法.   
   - 试试一些奇怪的东西，比如PyPy.  

5. Python工具：
- cProfile        : c语言实现的性能测试模块，接口和profile一样。
- line_profiler   : 可以统计每行代码的执行次数和执行时间等，时间单位为微妙。
- memory_profiler : 可以统计每行代码占用的内存大小。
- objgraph        : 一个实用模块，可以列出当前内存中存在的对象，可用于定位内存泄露。
- pycharm
